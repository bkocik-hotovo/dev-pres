<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>RAG</title>
    <link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/theme/black.css">
    <link rel="stylesheet" href="css/style.css">
    <style>
        body, .reveal p, .reveal ul, .reveal li, .reveal div, .reveal span {
            font-size: 0.85em;
            line-height: 1.4em;
        }
        h3 {
            font-size: 1.5em;
        }
    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">

        <section data-background-color="#141914" class="t-center">
            <h1 class="t-yellow">RAG</h1>
            <h3 class="t-gray">Retrieval-augmented generation</h3>
            <aside class="notes">
                Caute, vitam vas na tejto prezentacii, na ktorej sa budem venovat tematike RAGu, teda retrieval augmented generation. Povieme si o pred spracovani dat, nejakych jednoduchych workflowoch, moznych vylepseniach a nejakych limitaciach respektvice na co si treba davat pozor.
            </aside>
        </section>

        <section>
            <h3 class="t-yellow">Zopár základných pojmov</h3>
            <ul>
                <li><strong>Chunk:</strong> menšia časť textu pre ľahšie vyhľadávanie relevantných dát</li>
                <li><strong>Query/User Prompt:</strong> otázka, ktorú zadáva používateľ modelu</li>
                <li><strong>System Prompt:</strong> upravuje správanie modelu</li>
                <li><strong>Embedding:</strong> numerická reprezentácia chunku na porovnávanie sémantickej podobnosti</li>
                <li><strong>Token:</strong> základná jednotka textu spracovaná modelom</li>
                <li><strong>LLM Context Window:</strong> maximálny počet tokenov, ktoré vie model držať v svojej pamäti </li>
            </ul>
            <aside class="notes">
                Predtým než sa pustíme do podrobnejšieho vysvetľovania technológie RAG, rád by som vám predstavil niekoľko pojmov, s ktorými sa budeme počas prezentácie často stretávať.
                1. Chunk: Je menšia časť textu, ktorá vznikla jeho rozdelením za pomoci regexu alebo inej techniky. Služí na ľahšie vyhľadávanie relevantných dát.
                2. Query/User Prompt: Otázka či požiadavka, ktorú zadáva používateľ systému. Na základe query systém vyhľadá relevantné informácie a pripraví odpoveď.
                3. System Prompt: Inštrukcia zadaná systému alebo modelu, ktorá upravuje, ako má model interpretovať query a generovať odpoveď. System prompt sa používa na nastavenie spôsobu, akým model odpovedá.
                4. Embedding: Embedding je numerická reprezentácia chunka, ktorá umožňuje porovnávať sémantickú podobnosť medzi textami. Týmto spôsobom vieme vyhľadávať obsah na základe jeho významovej blízkosti.
                5. Token: Token je základná jednotka textu, ktorú model spracováva – môže predstavovať jedno slovo, časť slova alebo znak. Veľkosť modelu v tokens určuje, koľko textu dokáže model spracovať naraz.
                6. LLM Context Window: Kontextové okno LLM je maximálny počet tokenov, ktoré model dokáže spracovať súčasne. Tento limit určuje, koľko informácií je možné odoslať do modelu naraz, čo ovplyvňuje rozsah, ktorý model môže brať do úvahy pri generovaní odpovedí.            </aside>
        </section>

        <section>
            <h3 class="t-yellow">Čo je RAG?</h3>
            <p>Retrieval Augmented Generation (RAG) je technológia, ktorá kombinuje vyhľadávanie relevantných informácií s generatívnymi schopnosťami jazykových modelov. Pri retrievale získava najvhodnejšie informácie z externých zdrojov mimo modelu, ktoré sú následne použité jazykovým modelom (LLM) pre generovanie odpovedí na základe dodaného kontextu a otázky používateľa.</p>
            <aside class="notes">
                RAG, teda Retrieval Augmented Generation, je prístup, ktorý využíva jazykový model s integrovaným vyhľadávaním informácií, aby poskytol odpovede založené na aktuálnych dátach, nie len na statických znalostiach modelu. Pri použití retrieval kroku systém hľadá najrelevantnejšie časti z rôznych dátových zdrojov, pričom tieto časti sú zvyčajne rozdelené na menšie segmenty - chunky. Tieto chunky sa následne prevedú na vektorové reprezentácie a uložia do databázového indexu, ktorý umožňuje rýchle vyhľadávanie.

                Na rozdiel od tradičných jazykových modelov, ktoré sú založené na historických dátach, RAG umožňuje odpovede doplniť o aktuálny kontext, čo zvyšuje ich relevantnosť a presnosť.
            </aside>
        </section>


        <section>
            <h3 class="t-yellow">Spracovanie dát</h3>
            <ul>
                <li>pred implementáciou RAG systému potrebujeme pripraviť dáta.</li>
                <li>knižnice na spracovanie dát:
                    <ul>
                        <li><strong>Unstructured/PyMuPDF:</strong> na extrakciu textu a obrázkov</li>
                        <li><strong>Camelot:</strong> pre extrakciu údajov z tabuliek</li>
                    </ul>
                </li>
                <li>Rozdelenie na chunky, napr:
                    <ul>
                        <li>podľa regexu</li>
                        <li>sémanticky pomocou LangChain</li>
                    </ul>
                </li>
                <li>ukladanie vektorov chunkov do databázy pre efektívne vyhľadávanie, napr.:
                    <ul>
                        <li>PGVector</li>
                        <li>Chroma</li>
                        <li>Pinecone</li>
                    </ul>
                </li>
            </ul>
            <aside class="notes">
                Predtym nez ale zacneme implementovat nejaky RAG system, tak potrebujeme mat vhodne pripravene data. Na začiatku spracovania je potrebné najprv získať data z rôznych zdrojov, ako sú textové dokumenty alebo PDF. Na tento účel môžeme použiť knižnice ako Unstructured či PyMuPDF pre extrahovanie textu a obrázkov, alebo Camelot na získavanie údajov z tabuliek. Počas tohto procesu sa musime rozhodnut, ci chceme detekovať a extrahovat obrázky a či ich potrebujeme spracovať pomocou modelov na analýzu obrazu alebo nam staci len textova reprezentacia dat. Taktiez je otazne ci chceme pridavat k tymto ziskanym datam nejake dalsie metadata pre pripadne spatne vyhliadavanie odkial pochadadzaju generovane odpovede z LLM. Po tomto spracovani, ked uz máme všetky dáta na jednom mieste ako súvislý text, môžeme pokračovať ich rozdelením na menšie časti, tzv. chunky. Existuje viacero prístupov na tento účel, ja dam jeden jedodnuchy a jeden zlozitejsi pripad:
                1. Rozdeľovanie pomocou regulárnych výrazov (regex): Na základe znakov ako bodky, odrážky alebo odseky vieme rozdeliť text na jednotlive vety/odseky.
                2. Sémantické delenie: Na základe podobnosti významu vieme text rozdeliť na logické celky. Tento typ delenia, známy ako „semantic chunking,“ sa využíva na získanie kontextovo konzistentných častí textu a implementovať ho vieme na zaklade poznatkov od Grega Kamradta alebo vieme vyuzit existujucu implementaciu jeho riesenia v kniznici LangChain.
                Po rozdelení textu na chunky použijeme embedding model na získanie vektorovej reprezentácie každého chunki. Tento krok zabezpečí, že každá časť textu bude mať svoj unikátny vektor, čo nám umožní vyhľadávať a získavať relevantné chunky na základe podobnosti s query používateľa.
                Nakoniec uložíme tieto vektory do databázy podporujúcej vektorové vyhľadávanie, ako je napríklad PGVector, Chroma alebo Pinecone, čo nám zabezpečí efektívne a rýchle vyhľadávanie.             </aside>
        </section>


        <section>
            <h3 class="t-yellow">Jednoduchý workflow RAG</h3>
            <img src="dist/naive-300.png" alt="Jednoduchý RAG workflow" style="max-width: 100%; height: auto;">
            <aside class="notes">
                Aby som to zhrnul, takto nejako vyzera jednoduchy flow v RAG systeme. Na zaciatku flowu bude query pouzivatela. Na to aby sme vedeli najst k nej relevantne informacie, potrebujeme ju premenit do vektorovej podoby. To vieme urobit pomocou embedding modelu. Po tom co ziskame jej vektor, vieme jednoducho ziskat z databazy podobne vektory, ktore su spojene s chunkami textu a tie posleme spolu s query LLMku, ktore nam na zaklade poskytnutych chunkov odpovie.             </aside>
        </section>

        <section>
            <h3 class="t-yellow">Code snippet jednoduchého workflowu</h3>
            <pre><code class="python">
<span style="color: #569CD6">elements</span> = <span style="color: #DCDCAA">partition_pdf</span>(<span style="color: #CE9178">"example.pdf"</span>)

<span style="color: #6A9955"># Extract text content</span>
<span style="color: #569CD6">doc_content</span> = <span style="color: #CE9178">' '</span>.join(<span style="color: #569CD6">element.page_content</span>
                <span style="color: #569CD6">for</span> <span style="color: #569CD6">element</span> <span style="color: #569CD6">in</span> <span style="color: #569CD6">elements</span> <span style="color: #569CD6">if</span> <span style="color: #569CD6">element.page_content</span>)

<span style="color: #6A9955"># Semantic chunking and embedding</span>
<span style="color: #569CD6">text_splitter</span> = <span style="color: #DCDCAA">SemanticChunker</span>(<span style="color: #DCDCAA">OpenAIEmbeddings</span>())
<span style="color: #569CD6">chunks</span> = <span style="color: #569CD6">text_splitter.split_text</span>(<span style="color: #569CD6">doc_content</span>)
<span style="color: #569CD6">embeddings</span> = <span style="color: #569CD6">text_splitter.embeddings.embed_documents</span>(<span style="color: #569CD6">chunks</span>)

<span style="color: #6A9955"># Insert chunks and embeddings into `pgvector` table</span>

<span style="color: #6A9955"># Retrieve 5 similar chunks by cosine similarity</span>
<span style="color: #569CD6">def</span> <span style="color: #DCDCAA">retrieve_similar_chunks</span>(<span style="color: #569CD6">query_embedding</span>, <span style="color: #569CD6">top_k</span>=<span style="color: #569CD6">5</span>):
    <span style="color: #569CD6">cursor.execute</span>(
        <span style="color: #CE9178">"""
        SELECT chunk
        FROM embeddings_table
        ORDER BY embedding <=> %s
        LIMIT %s
        """</span>,
        (<span style="color: #569CD6">query_embedding</span>, <span style="color: #569CD6">top_k</span>)
    )
    <span style="color: #569CD6">return</span> [<span style="color: #569CD6">row</span>[<span style="color: #569CD6">0</span>] <span style="color: #569CD6">for</span> <span style="color: #569CD6">row</span> <span style="color: #569CD6">in</span> <span style="color: #569CD6">cursor.fetchall</span>()]

<span style="color: #6A9955"># Example embedding of query for retrieval</span>
<span style="color: #569CD6">user_prompt</span> = <span style="color: #CE9178">"What is RAG?"</span>
<span style="color: #569CD6">query_embedding</span> = <span style="color: #569CD6">text_splitter.embeddings.embed_query</span>([<span style="color: #569CD6">user_prompt</span>])
<span style="color: #569CD6">retrieved_chunks</span> = <span style="color: #569CD6">retrieve_similar_chunks</span>(<span style="color: #569CD6">query_embedding</span>)

<span style="color: #6A9955"># Define system prompt and LLM</span>
<span style="color: #569CD6">prompt</span> = <span style="color: #DCDCAA">ChatPromptTemplate.from_messages</span>(
    [
        (<span style="color: #CE9178">"system"</span>, <span style="color: #CE9178">"You are a helpful POC assistant that helps summarize
                retrieved content relevant to the user prompt."</span>),
        (<span style="color: #CE9178">"human"</span>, <span style="color: #CE9178">"{retrieved_chunks}"</span>),
        (<span style="color: #CE9178">"human"</span>, <span style="color: #CE9178">"{user_prompt}"</span>)
    ]
)
<span style="color: #569CD6">llm</span> = <span style="color: #DCDCAA">OpenAI</span>()

<span style="color: #6A9955"># Chain prompt and LLM invocation</span>
<span style="color: #569CD6">chain</span> = <span style="color: #569CD6">prompt</span> | <span style="color: #569CD6">llm</span>
<span style="color: #569CD6">response</span> = <span style="color: #569CD6">chain.invoke</span>(
    {
        <span style="color: #CE9178">"retrieved_chunks"</span>: <span style="color: #569CD6">'\n'.join</span>(<span style="color: #569CD6">retrieved_chunks</span>),
        <span style="color: #CE9178">"user_prompt"</span>: <span style="color: #569CD6">user_prompt</span>
    }
)
</code></pre>
            <aside class="notes">
                Tu si mozeme tak narychlo ukazat zjednodusenu implementaciu RAG flowu. Prvým krokom je spracovanie PDF dokumentu pomocou funkcie partition_pdf, ktorá rozdelí PDF na jednotlivé časti, pričom každý segment uloží ako samostatný prvok pre jednoduchšie vyhľadávanie. Následne všetky textové prvky spojíme do jedného reťazca, čím získame textový obsah dokumentu pripravený na ďalšie spracovanie.
                Ďalej sa zameriavame na delenie textu a jeho vektorovú reprezentáciu. Pomocou nástroja SemanticChunker získa každá časť textu tzv. embeddings, čo sú numerické reprezentácie textu, ktoré umožňujú porovnávať podobnosť medzi segmentami na základe ich významu. Tieto embeddings vložíme do databázy, napríklad pomocou knižnice pgvector.
                Keď je používateľova otázka zadaná, napríklad “Čo je to RAG?”, prevedieme túto otázku na embedding a použijeme funkciu retrieve_similar_chunks na vyhľadanie piatich najpodobnejších častí textu, ktoré zodpovedajú otázke. Tieto relevantné chunky pošleme spolu s pôvodnou otázkou systému.
                V poslednej fáze použijeme LLM, ktorému zadáme systémový prompt definujúci, ako má model odpovedať, a užívateľský prompt s query a získanými chunkmi. Tento reťazec nám vráti odpoveď, ktorá spája relevantné informácie z dokumentu so zadávateľovou otázkou.            </aside>
            </aside>
        </section>

        <section>
            <h3 class="t-yellow">Komplexnejší workflow RAG</h3>
            <div style="display: flex; align-items: flex-start; gap: 20px; margin-top: 20px;">
                <div style="flex: 1; padding-right: 10px;">
                    <ul>
                        <li><strong>Query Transformation:</strong> LLM rozkladá zložitý dotaz na sub-queries</li>
                        <li><strong>Query Routing:</strong> nasmerovanie dotazu k relevantnému zdroju</li>
                        <li><strong>Hybrid Retrieval:</strong> spojenie semantického vyhľadávania s kľúčovými slovami</li>
                        <li><strong>Chunk Re-ranking:</strong> najrelevantnejšie chunky sú prioritizované</li>
                        <li><strong>Summary Index:</strong> rýchle vyhľadávanie v sumároch dokumentov</li>
                    </ul>
                </div>
                <div style="flex: 1; padding-left: 10px;">
                    <img src="dist/advanced-300.png" alt="Komplexný RAG workflow" style="width: 100%; max-width: 500px; height: auto;">
                </div>
            </div>
            <aside class="notes">
                Na tomto slajde máme zobrazený pokročilejší workflow RAG systému, ktorý obsahuje niekoľko doplňujúcich procesov pre efektívnejšie vyhľadávanie a spracovanie queries.
                1. Query Transformation: pri query transformation vyuzivme llm na preformulovanie alebo rozloženie používateľskej query tak, aby lepšie zodpovedala našim datam. Ak je query komplexna, model ju môže rozdeliť na niekoľko menších sub-queries. Napríklad, ak sa spýtame ci viac kg drepuje Viktor alebo Marek“, model to moze rozdeliť na dve jednoduchšie queries ako: „Koľko kg drepuje Viktor?“ a „Koľko kg drepuje Marek?“. Tieto sub-queries sa spracujú samostatne a výsledky sa nakoniec spoja, čím získa používateľ celistvú odpoveď.
                2. Query Routing: Ak napríklad máme dáta o rôznych mestách, pomocou embeddingu query vieme identifikovat najviac relevantne zdroje, cim eliminujeme potrebu prehľadávať všetky databázy a zameria sa len na tie relevantné.
                3. Hybrid Retrieval: Vieme spojit semanticke vyhliadavanie pomocou vektorov s vyhliadavanim pomocou klucovych slov, ktore vieme my dopredu zadefinovat alebo ich ziskat pomocou spracovania query LLMkom
                4. Chunk Re-ranking: Chunky, ktore posielame LLM vieme zoradit tak, aby najviac relevantne boli na zaciatku a konci user prompty, kedze viacere studie uz ukazali, ze llmko sa najviac pozera na zaciatok a koniec spravy.
                5. Summary Index: umožňuje nám najprv prehľadať sumáre dokumentov, aby sme našli tie najrelevantnejšie, a až potom podrobne prehľadať samotné chunky. Týmto dvojstupňovým prístupom vieme rýchlo zúžiť rozsah vyhľadávania na najrelevantnejšie časti a zabezpečiť presnejšie a rýchlejšie odpovede.
            </aside>
        </section>


        <section>
            <h3 class="t-yellow">Nastavenie system promptu</h3>
            <ul>
                <li><strong>Vymedzenie dát pre odpovede:</strong> model odpovedá iba na základe poskytnutých kontextových údajov, s rozlíšením medzi poskytnutými chunkmi a jeho vlastnými poznatkami</li>
                <li><strong>Špecifikácia vstupu a výstupu:</strong> presné definovanie, ako model spracuje dotaz, vrátane štruktúry user query a relevantných chunkov, aby nedošlo k nesprávnemu vyloženiu query</li>
                <li><strong>Štandardizovaný výstup:</strong> ak pracujete so štruktúrovanými výstupmi (napr. JSON schema), je dôležité presne nastaviť formát očakávaného výstupu pre konzistentné výsledky</li>
            </ul>
            <aside class="notes">
                Na tejto časti sa zameriame na nastavenie systému tak, aby poskytoval čo najpresnejšie a najspoľahlivejšie odpovede. V RAG systémoch, kde sa model opiera o externé zdroje, je dôležité, aby odpovede boli ukotvené vo vloženom kontexte a model sa vyhýbal takzvaným „halucináciám“ – teda odpovediam, ktoré nie sú podložené reálnymi dátami. Existuje niekoľko prístupov, ktoré nám môžu pomôcť dosiahnuť túto konzistenciu:
                1. Vymedzenie dat pouzitych na generovanie odpovede: aby sme predosli halucinovaniu, môžeme v system prompte jednoznačne určiť, aby model odpovedal len na základe poskytnutých kontextových údajov. Ak je v poriadku, aby odpovedal aj na zaklade toho co uz je nauceny, tak by stalo za to specifikovat aby v odpovedi diferencoval medzi castami odpovede zalozenymi na poskytnutych chunkoch a na castiach o ktorych mal on vlastne informacie. Vieme takto lepsie prist na mozne halucinovanie/chyby v odpovediach
                2. Uvedenie špecifikácií pre prichádzajúci vstup a očakávaný výstup: Je dôležité presne definovať, ako má model interpretovať vstupné údaje používateľa a aký štýl odpovede sa očakáva. Co sa tyka vstupu tak sa oplati spomenut ako budete strukturovat user query, kde sa bude nachadzat user query a kde budu relevantne chunky, aby nedoslo k zlemu vylozeniu user query. Ak dalej spracovavate jeho vystup, tak bez presnych specifikacie bude stale aspon trocha rozlicny(bohuzial sa to niekedy deje aj napriek presnym specifikaciam ako je napriklad json schema odpovede).
            </aside>
        </section>

        <section>
            <h3 class="t-yellow">Use cases a limitácie</h3>
            <ul>
                <li><strong>Praktické použitia:</strong> ideálne pre prostredia, kde sa často vyhľadávajú informácie v dokumentoch – efektívne pri statických dátach, kde môžu byť vopred pripravené prompty</li>
                <li><strong>Obmedzenia úložiska:</strong> veľkosť jedného 1536 dimenzionalneho vektoru je približne 6 KB, čo zvyšuje náročnosť na úložisko</li>
                <li><strong>Výber embedding modelu:</strong> správny embedding model je kľúčový, keďže modely sa špecializujú na rôzne prípady použitia</li>
                <li><strong>Prompt engineering:</strong> efektivitu môže znížiť nedostatočná znalosť používateľa v správnom formulovaní dotazov, čo si vyžaduje nejaké školenie v oblasti prompt engineeringu</li>
            </ul>
            <aside class="notes">
                Na zaver by sme si povedali o nejakych realnych use casoch a limitaciach. Co sa tyka pouziti tak taketo systemy su vhodne a setria vela casu na miestach kde casto musite prehliadavat dokumenty a hladat v nich informacie. O to lepsie je, ked su tie data staticke, takze si viete dopredu predpripravit nejake prompty/use casy a nemusite sa spoliehat na pouzivatela, ze vie ako sa ma spravne pytat. Problemom pri takychto systemoch je, ze su velmi narocne na ulozisko, kedze velkost jednoho vectoru s dimenzionalitou 1536, je okolo 6 kilobytov. Dalsim problemom je vyber spravneho embedding modelu pre vas use case. Embedding modely su casto dobre na specificke pouzitia a velmi ovplyvnuju relevantnost vybratych chunkov z databazy. Najvacsie porovnanie tychto modelov robi momentalne hugging face. Ak uz mate toto vsetko pokryte tak efektivnost RAGu sa vie velmi znizit aj na strane usera, ktory sa nemusi vediet spravne pytat, takze tam je potrebne skolenie na nejaky prompt engineering.            </aside>
        </section>

        <section>
            <h2 class="t-yellow">Ďakujem za pozornosť</h2>
        </section>
    </div>
</div>

<script src="https://unpkg.com/reveal.js/dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script>
    Reveal.initialize({
        plugins: [RevealNotes],
    });
</script>
</body>
</html>