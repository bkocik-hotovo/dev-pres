<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>RAG</title>
    <link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/theme/black.css">
    <link rel="stylesheet" href="css/style.css">
    <style>
        body, .reveal p, .reveal ul, .reveal li, .reveal div, .reveal span {
            font-size: 0.85em;
            line-height: 1.4em;
        }
        h3 {
            font-size: 1.5em;
        }
    </style>
</head>
<body>
<div class="reveal">
    <div class="slides">

        <section data-background-color="#141914" class="t-center">
            <h1 class="t-yellow">RAG</h1>
            <h3 class="t-gray">Retrieval and generation</h3>
        </section>

        <section>
            <h3 class="t-yellow">Zopár základných pojmov</h3>
            <ul>
                <li><strong>Chunk:</strong> menšia časť textu pre ľahšie vyhľadávanie relevantných dát</li>
                <li><strong>Query/User Prompt:</strong> otázka, ktorú zadáva používateľ systému</li>
                <li><strong>System Prompt:</strong> inštrukcia systému, ktorá upravuje správanie modelu</li>
                <li><strong>Embedding:</strong> numerická reprezentácia chunka na porovnávanie sémantickej podobnosti</li>
                <li><strong>Token:</strong> základná jednotka textu spracovaná modelom</li>
                <li><strong>LLM Context Window:</strong> maximálny počet tokenov, ktoré model spracuje naraz</li>
            </ul>
        </section>

        <section>
            <h3 class="t-yellow">Čo je RAG?</h3>
            <p>RAG využíva aktuálne informácie v systéme, aby poskytoval čo najviac relevantné odpovede. Retrieval získava relevantné informácie z dostupných zdrojov a následne sa generuje odpoveď v LLM na základe poskytnutých chunkov a používateľského promptu.</p>
        </section>

        <section>
            <h3 class="t-yellow">Spracovanie dát</h3>
            <ul>
                <li>pred implementáciou RAG systému potrebujeme pripraviť dáta.</li>
                <li>knižnice na spracovanie dát:
                    <ul>
                        <li><strong>Unstructured/PyMuPDF:</strong> na extrakciu textu a obrázkov</li>
                        <li><strong>Camelot:</strong> pre extrakciu údajov z tabuliek</li>
                    </ul>
                </li>
                <li>Rozdelenie na chunky, napr:
                    <ul>
                        <li>podľa regexu</li>
                        <li>sémanticky pomocou LangChain</li>
                    </ul>
                </li>
                <li>ukladanie vektorov chunkov do databázy pre efektívne vyhľadávanie, napr.:
                    <ul>
                        <li>PGVector</li>
                        <li>Chroma</li>
                        <li>Pinecone</li>
                    </ul>
                </li>
            </ul>
        </section>


        <section>
            <h3 class="t-yellow">Jednoduchý workflow RAG</h3>
            <img src="dist/naive-300.png" alt="Jednoduchý RAG workflow" style="max-width: 100%; height: auto;">
        </section>

        <section>
            <h3 class="t-yellow">Code snippet jednoduchého workflowu</h3>
            <pre><code data-trim>
elements = partition_pdf("example.pdf")

# Extract text content
doc_content = ' '.join(element.page_content
                for element in elements if element.page_content)

# Semantic chunking and embedding
text_splitter = SemanticChunker(OpenAIEmbeddings())
chunks = text_splitter.split_text(doc_content)
embeddings = text_splitter.embeddings.embed_documents(chunks)

# Insert chunks and embeddings into `pgvector` table

# Retrieve 5 similar chunks by cosine similarity
def retrieve_similar_chunks(query_embedding, top_k=5):
    cursor.execute(
        """
        SELECT chunk
        FROM embeddings_table
        ORDER BY embedding <=> %s
        LIMIT %s
        """,
        (query_embedding, top_k)
    )
    return [row[0] for row in cursor.fetchall()]

# Example embedding of query for retrieval
user_prompt = "What is RAG?"
query_embedding = text_splitter.embeddings.embed_query([user_prompt])
retrieved_chunks = retrieve_similar_chunks(query_embedding)

# Define system prompt and LLM
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful POC assistant that helps summarize
                retrieved content relevant to the user prompt."),
        ("human", "{retrieved_chunks}"),
        ("human", "{user_prompt}")
    ]
)
llm = OpenAI()

# Chain prompt and LLM invocation
chain = prompt | llm
response = chain.invoke(
    {
        "retrieved_chunks": '\n'.join(retrieved_chunks),
        "user_prompt": user_prompt
    }
)
            </code></pre>
        </section>

        <section>
            <h3 class="t-yellow">Komplexnejší workflow RAG</h3>
            <div style="display: flex; align-items: flex-start; gap: 20px; margin-top: 20px;">
                <div style="flex: 1; padding-right: 10px;">
                    <ul>
                        <li><strong>Query Transformation:</strong> LLM rozkladá zložitý dotaz na sub-queries</li>
                        <li><strong>Query Routing:</strong> nasmerovanie dotazu k relevantnému zdroju</li>
                        <li><strong>Hybrid Retrieval:</strong> spojenie semantického vyhľadávania s kľúčovými slovami</li>
                        <li><strong>Chunk Re-ranking:</strong> najrelevantnejšie chunky sú prioritizované</li>
                        <li><strong>Summary Index:</strong> rýchle vyhľadávanie v sumároch dokumentov</li>
                    </ul>
                </div>
                <div style="flex: 1; padding-left: 10px;">
                    <img src="dist/advanced-300.png" alt="Komplexný RAG workflow" style="width: 100%; max-width: 500px; height: auto;">
                </div>
            </div>
        </section>


        <section>
            <h3 class="t-yellow">Nastavenie system promptu</h3>
            <ul>
                <li><strong>Vymedzenie dát pre odpovede:</strong> model odpovedá iba na základe poskytnutých kontextových údajov, s rozlíšením medzi poskytnutými chunkmi a jeho vlastnými poznatkami</li>
                <li><strong>Špecifikácia vstupu a výstupu:</strong> presné definovanie, ako model spracuje dotaz, vrátane štruktúry user query a relevantných chunkov, aby nedošlo k nesprávnemu vyloženiu query</li>
                <li><strong>Štandardizovaný výstup:</strong> ak pracujete so štruktúrovanými výstupmi (napr. JSON schema), je dôležité presne nastaviť formát očakávaného výstupu pre konzistentné výsledky</li>
            </ul>
        </section>

        <section>
            <h3 class="t-yellow">Use cases a limitácie</h3>
            <ul>
                <li><strong>Praktické použitia:</strong> ideálne pre prostredia, kde sa často vyhľadávajú informácie v dokumentoch – efektívne pri statických dátach, kde môžu byť vopred pripravené prompty</li>
                <li><strong>Obmedzenia úložiska:</strong> veľkosť jedného vektoru s dimenziou 1536 je približne 6 KB, čo zvyšuje náročnosť na úložisko</li>
                <li><strong>Výber embedding modelu:</strong> správny embedding model je kľúčový, keďže modely sa špecializujú na rôzne prípady použitia</li>
                <li><strong>Prompt engineering:</strong> efektivitu môže znížiť nedostatočná znalosť používateľa v správnom formulovaní dotazov, čo si vyžaduje nejaké školenie v oblasti prompt engineeringu</li>
            </ul>
        </section>

        <section>
            <h2 class="t-yellow">Ďakujem za pozornosť</h2>
        </section>
    </div>
</div>

<script src="https://unpkg.com/reveal.js/dist/reveal.js"></script>
<script>
    Reveal.initialize();
</script>
</body>
</html>